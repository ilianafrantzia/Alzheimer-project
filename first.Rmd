---
title: "DSC 532- Alzheimers classification project"
output:
  pdf_document: default
  html_document: default
date: "2025-03-31"
---

This project uses the Alzheimer's Prediction Dataset, which includes health and demographic information from over 74,000 people across 20 different countries. The goal is to explore what factors might be linked to Alzheimer's disease and to build models that can help predict whether someone is likely to develop it.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


!!!!
We might need to scale for kNN or other methods (LDA,...)

### Import Libraries
```{r libraries,echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(moments)
library(nortest)
library(corrplot)
library(caret)
library(tidyr)

```

Load the alzheimers_prediction_dataset and preview the first 6 rows of the dataset.

```{r load-data, message=FALSE}
data <- read.csv("alzheimers_prediction_dataset.csv")
head(data)
```
The dataset includes features like: 
```{r}
colnames(data)
``` 

Checking the data types and structure of the dataset.
```{r}
str(data)
```

As observed above, most of our categorical features are of type 'character', so we will convert them to factors.

Statistical summary of numerical variables.
```{r}
summary(data)
```
- The age range is between 50 to 94 and the average age is 72.
- Education level is between 0 to 19 ,could represent years or level of education,where the average is around 9.
- BMI range between 18.5–35 and the average is 26.78.
- Cognitive test score is between 30 to 99 with average around 65.
All observations are logical and there are no data entry.
We will continue by checking if there are any missing or duplicated values.

```{r}
# Total number of missing values in the dataset
sum(is.na(data))

```
We do not have any missing values.

```{r}

# Number of duplicated rows
sum(duplicated(data))

```
We do not have any duplicates.

```{r}

# Check unique values for categorical columns
cat_cols <- names(data)[sapply(data, is.character)]

# Apply unique() to each categorical column and name the output
unique_values <- lapply(data[cat_cols], unique)

# Display the unique values nicely
unique_values


```
WE checked the values of all the categorical columns to ensure that there are not any data entry errors or misspelling.

```{r turn to factor}
data <- data %>%
  mutate(across(where(is.character), as.factor))
```

### Visualize numerical columns

```{r hist-age}
ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth=1,fill = "lightblue", color = "black") +
  labs(title = "Histogram of Age", x = "Age", y = "Count") +
  theme_minimal()
```
We observed that the age distribution is balanced.

The 10 most common ages of our datasets are:
```{r age-counts}
age_counts<-table(data$Age)
# Sort in descending order
sorted_age_counts <- sort(age_counts, decreasing = TRUE)

# Show top results
head(sorted_age_counts, 10)

# Sort in ascending order
asorted_age_counts <- sort(age_counts)

# Show top results
head(asorted_age_counts, 10)

```
Most people are 72 years old and the least are 68 years old.


```{r hist-bmi}
ggplot(data, aes(x = BMI)) +
  geom_histogram(binwidth=1,fill = "lightblue", color = "black") +
  labs(title = "Histogram of BMI", x = "BMI", y = "Count") +
  theme_minimal()
```

Also, the BMI distribution is balanced.

```{r hist-education}
ggplot(data, aes(x = Education.Level)) +
  geom_histogram(binwidth=1,fill = "lightblue", color = "black") +
  labs(title = "Histogram of Education Level", x = "Years of Education", y = "Count") +
  theme_minimal()
```

The education level distribution is also balanced.

```{r hist-cognitive-score}
ggplot(data, aes(x = Cognitive.Test.Score)) +
  geom_histogram(binwidth=1,fill = "lightblue", color = "black") +
  labs(title = "Histogram of Cognitive Test Score", x = "Score", y = "Count") +
  theme_minimal()
```

The distribution of cognitive test score is balanced.


### Visualize categorical columns

```{r bar-gender}
ggplot(data, aes(x = Gender)) +
  geom_bar(binwidth=1,fill = "lightblue") +
  labs(title = "Distribution of Gender", x = "Gender", y = "Count") +
  theme_minimal()
```

The gender distribution is balanced,equal number of female and male.


```{r bar-smoking}
ggplot(data, aes(x = Smoking.Status)) +
  geom_bar(binwidth=1,fill = "lightblue") +
  labs(title = "Smoking Status Distribution", x = "Smoking Status", y = "Count") +
  theme_minimal()
```

The smoking status distribution is also balanced.

```{r bar-family-history}
ggplot(data, aes(x = Family.History.of.Alzheimer.s)) +
  geom_bar(binwidth=1,fill = "lightblue") +
  labs(title = "Family History of Alzheimer's", x = "Family History", y = "Count") +
  theme_minimal()
```

The family history of Alzheimer's distribution is unbalanced,most people do not have family history of Alzheimer's.

```{r bar-activity}
ggplot(data, aes(x = Physical.Activity.Level)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Physical Activity Level", x = "Activity Level", y = "Count") +
  theme_minimal()
```

The plysical activity distribution is balanced.

```{r bar-alcohol}
ggplot(data, aes(x = Alcohol.Consumption)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Alcohol Consumption", x = "Alcohol Use", y = "Count") +
  theme_minimal()
```

The alcohol consumption distribution is balanced.

```{r bar-diabetes}
ggplot(data, aes(x = Diabetes)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Diabetes Status", x = "Diabetes", y = "Count") +
  theme_minimal()
```

The diabetes distribution is unbalanced ,where most people do not have diabetes.

```{r bar-hypertension}
ggplot(data, aes(x = Hypertension)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Hypertension Status", x = "Hypertension", y = "Count") +
  theme_minimal()
```

The hypertension distribution is unbalanced,most patients do not have hypertension.

```{r bar-cholesterol}
ggplot(data, aes(x = Cholesterol.Level)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Cholesterol Level", x = "Cholesterol", y = "Count") +
  theme_minimal()
```

The cholesterol distribution is also not balanced and most people have normal cholesterol level.

```{r bar-depression}
ggplot(data, aes(x = Depression.Level)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Depression Level", x = "Depression", y = "Count") +
  theme_minimal()
```

The depression level distribution is balanced.

```{r bar-sleep}
ggplot(data, aes(x = Sleep.Quality)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Sleep Quality", x = "Sleep Quality", y = "Count") +
  theme_minimal()
```

The sleep quality distribution is balanced.

```{r bar-diet}
ggplot(data, aes(x = Dietary.Habits)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Dietary Habits", x = "Diet", y = "Count") +
  theme_minimal()
```

The dietary habits distribution is also balanced.

```{r bar-pollution}
ggplot(data, aes(x = Air.Pollution.Exposure)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Air Pollution Exposure", x = "Pollution Exposure", y = "Count") +
  theme_minimal()
```

The air pollution exposure distribution is balanced.

```{r bar-employment}
ggplot(data, aes(x = Employment.Status)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Employment Status", x = "Employment", y = "Count") +
  theme_minimal()
```

The employment distribution is also balanced.

```{r bar-marital}
ggplot(data, aes(x = Marital.Status)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Marital Status", x = "Marital Status", y = "Count") +
  theme_minimal()
```

The marital status distribution is balanced.

```{r bar-apoe}
ggplot(data, aes(x = Genetic.Risk.Factor..APOE.ε4.allele.)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Genetic Risk Factor: APOE-ε4 Allele",
       x = "APOE-ε4 Status",
       y = "Count") +
  theme_minimal()
```

The genetic risk distribution is unbalanced where most people have no genetic risk factor APOE-ε4 Allele.

```{r bar-income}
ggplot(data, aes(x = Income.Level)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Income Level",
       x = "Income Level",
       y = "Count") +
  theme_minimal()
```

The distribution of income level is balanced.

```{r bar-social-engagement}
ggplot(data, aes(x = Social.Engagement.Level)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Social Engagement Level",
       x = "Social Engagement Level",
       y = "Count") +
  theme_minimal()
```

The distribution of social engagement level is balanced.


```{r bar-stress}
ggplot(data, aes(x = Stress.Levels)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Stress Levels",
       x = "Stress Level",
       y = "Count") +
  theme_minimal()
```

The distribution of stress levels is balanced.

```{r bar-living-area}
ggplot(data, aes(x = Urban.vs.Rural.Living)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Living Area: Urban vs Rural",
       x = "Living Area",
       y = "Count") +
  theme_minimal()
```

The living area distribution is balanced.


```{r bar-alzheimers-diagnosis}
ggplot(data, aes(x = Alzheimer.s.Diagnosis)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Distribution of Alzheimer's Diagnosis",
       x = "Diagnosis",
       y = "Count") +
  theme_minimal()
```

The distribution of alzheimer's diagnosis is unbalanced,most people have not been diagnosed with alzheimer.


```{r diagnosis-percentages}
# Count of each diagnosis category
table(data$Alzheimer.s.Diagnosis)

# Proportion (percentage) of each category
round(prop.table(table(data$Alzheimer.s.Diagnosis)) * 100, 2)
```
The distribution of Alzheimer’s diagnosis shows that approximately 58% of individuals are not diagnosed with the disease, while 42% are. 


Since we have a mid balanced dataset, we will try to balance it by resampling. We will then compare the results from the original dataset and the resampled dataset to see which one performs better.

```{r split}

# Ensure target is a factor
data$Alzheimer.s.Diagnosis <- as.factor(data$Alzheimer.s.Diagnosis)

# Create an 80-20 train-test split
set.seed(1)
train_index <- createDataPartition(data$Alzheimer.s.Diagnosis, p = 0.8, list = FALSE)

train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Separate predictors and response
X_train <- train_data[, -which(names(train_data) == "Alzheimer.s.Diagnosis")]
y_train <- train_data$Alzheimer.s.Diagnosis

# Apply upsampling
upsampled_train <- upSample(x = X_train, y = y_train, yname = "Alzheimer.s.Diagnosis")

#Model on original
model_orig <- train(Alzheimer.s.Diagnosis ~ ., data = train_data, method = "glm", family = "binomial")

#Model on resampled
model_up <- train(Alzheimer.s.Diagnosis ~ ., data = upsampled_train, method = "glm", family = "binomial")

# Predictions
pred_orig <- predict(model_orig, test_data)
pred_up <- predict(model_up, test_data)

# Confusion Matrices
confusionMatrix(pred_orig, test_data$Alzheimer.s.Diagnosis)
confusionMatrix(pred_up, test_data$Alzheimer.s.Diagnosis)

```

Recall / Sensitivity	✅ Critical — You want to catch as many actual cases as possible (minimize FN).

In Alzheimer’s prediction, a false negative (saying someone doesn’t have Alzheimer’s when they do) can delay treatment and support. So:
- You prefer false positives (extra caution) over false negatives (missed diagnosis).
- Therefore, your original model may be better aligned with your goal, despite the class imbalance, because it has higher sensitivity.


What You Can Do Next:
1. Try other models like Random Forest or SVM with class weights or upsampling, and compare their sensitivity.
2. Consider a custom threshold to favor recall (e.g., adjust cutoff from 0.5 to 0.4 if using predicted probabilities).
3. Evaluate models using ROC curves and precision-recall curves.


### Boxplots for Numerical Features vs Target

```{r boxplots for numeric with target}

numeric_vars <- names(data)[sapply(data, is.numeric)]

for (var in numeric_vars) {
  print(
    ggplot(data, aes_string(x = "Alzheimer.s.Diagnosis", y = var, fill = "Alzheimer.s.Diagnosis")) +
      geom_boxplot() +
      labs(title = paste("Boxplot of", var, "by Alzheimer Diagnosis"), x = "Diagnosis.s.Diagnosis", y = var) +
      theme_minimal()
  )
}

```

### Stacked Bar Plots for Categorical Features

```{r barplots for categorical features with target}

cat_vars <- names(data)[sapply(data, is.factor)]

for (var in cat_vars) {
  print(
    ggplot(data, aes_string(x = var, fill = "Alzheimer.s.Diagnosis")) +
      geom_bar(position = "fill") +
      labs(title = paste("Proportion of Alzheimer Diagnosis by", var), y = "Proportion") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  )
}


```

### Correlation plot for numerical features

```{r corr plot}

numeric_data <- data[, sapply(data, is.numeric)]
# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Plot it using corrplot
corrplot(cor_matrix,
         method = "color",       # Use colored squares
         type = "upper",         # Show only upper triangle
         order = "hclust",       # Cluster similar variables
         tl.cex = 0.8,           # Text label size
         number.cex = 0.7,       # Correlation number size
         addCoef.col = "black",  # Add correlation values
         diag = FALSE)           # Don’t show diagonal

```

So the correlation plot tells us that there is not relationship between the numeric variables.

```{r corr plot 2}

# Convert the target to numeric: 1 for "Yes", 0 for "No"
data$Alz_numeric <- ifelse(data$Alzheimer.s.Diagnosis == "Yes", 1, 0)

numeric_vars <- names(data)[sapply(data, is.numeric)]
# Exclude the binary version of the target
numeric_vars <- setdiff(numeric_vars, "Alz_numeric")

# Calculate correlation with Alz_numeric
pb_cor <- sapply(data[, numeric_vars], function(x) cor(x, data$Alz_numeric, method = "pearson", use = "complete.obs"))

# Convert to data frame
pb_df <- data.frame(Feature = names(pb_cor), PointBiserial_Correlation = pb_cor)
pb_df <- pb_df[order(abs(pb_df$PointBiserial_Correlation), decreasing = TRUE), ]  # Sort by strength

pb_df


```

To explore the relationship between numerical features and Alzheimer diagnosis, point-biserial correlations were calculated. Among all numeric variables, Age showed the strongest association with Alzheimer’s, with a moderate positive correlation of 0.42. This indicates that older individuals in the dataset are more likely to be diagnosed with Alzheimer’s — a finding that aligns well with real-world medical understanding. In contrast, Education Level, BMI, and Cognitive Test Score exhibited near-zero correlations, suggesting little to no linear relationship with the diagnosis in this dataset. While these variables may not show a strong direct correlation, they could still contribute to predictive performance in models that can capture nonlinear or interaction effects, such as decision trees or random forests. Therefore, these variables will be retained for modeling despite their low individual correlation.


### Scale dataset for modeling

We will scale the numeric variables in our dataset only when needed for specific models like Logistic Regression, k-Nearest Neighbors, SVM or LDA / QDA. Models like Decision Trees, Random Forest or Naive Bayes do not need scaling.

```{r scale}

# Identify numeric columns (excluding the target and any other non-predictors)
numeric_vars <- names(data)[sapply(data, is.numeric)]
numeric_vars <- setdiff(numeric_vars, "Alz_numeric")  # If you created this earlier

# Extract the numeric data
data_numeric <- data[, numeric_vars]

# Create scaling object
scaling_model <- preProcess(data_numeric, method = c("center", "scale"))

# Apply scaling
scaled_numeric <- predict(scaling_model, data_numeric)

# Keep categorical columns and target
non_numeric <- data[, !(names(data) %in% numeric_vars)]

# Combine scaled numeric with original non-numeric
data_scaled <- cbind(scaled_numeric, non_numeric)

summary(data_scaled[, numeric_vars])  # Mean should be ~0, SD ~1
sapply(data_scaled[, numeric_vars], sd)

```

From the results above, we confirm that the scaling was done correctly since the mean is 0 and the sd is equal to 1.





ELENI------

```{r predictions}
glm.fit <- glm(Alzheimer.s.Diagnosis ~ ., data = train_data, family = binomial)
summary(glm.fit)


# This helps automatically drop variables that don’t improve the model.
# Stepwise model selection (both directions)
step_model <- step(glm.fit, direction = "both")

# View summary of the updated model
summary(step_model)

install.packages("car")  # only do this once
library(car)
vif(step_model)

pred_probs <- predict(step_model, type = "response")
# Step 1: Create default class vector (assume Negative)
glm_pred <- rep("Negative", length(pred_probs))

# Step 2: Set to Positive if prob > 0.5
glm_pred[pred_probs > 0.5] <- "Positive"

# Step 3: Check a few predictions
head(glm_pred)

# Step 4: Compare to actual
table(Predicted = glm_pred, Actual = data$Alzheimer.s.Diagnosis)
```

Commenting on the logistic regression output: 

- Model type: Binary logistic regression using glm() with family = binomial
- Target variable: Alzheimer's Diagnosis
- Sample size: 74,283 observations
- AIC (Akaike Information Criterion): 80,503 — lower AIC is better for model comparison, but on its own, not interpretable.
- Residual deviance: 80,393 — compared to null deviance (100,742), this shows your model explains some variability, which is good.
- Fisher scoring iterations: 4 — indicates convergence was achieved quickly.


Some conclusions:

1. Risk increases with age.
2. Higher likelihood in Brazil, India, Mexico, Russia, South Africa compared to the baseline (likely the first country alphabetically). - (Positive coefficients)
3. Lower likelihood in Canada, Japan, Norway, Sweden.  
4. Genetic Risk Factor (APOE ε4 allele): Yes -> Strongest predictor in your model. Significantly increases risk.


In the code above we simplified the logistic regression model using stepwise selection, which means that 
our final model includes only the following predictors:

- Country  
- Age  
- Family.History.of.Alzheimer.s  
- Genetic.Risk.Factor..APOE.ε4.allele.  
- Urban.vs.Rural.Living  


✅ Accuracy is decent (71.6%)
🟡 Sensitivity (60.9%) is moderate — your model catches about 61% of actual Alzheimer's cases.
🟢 Specificity (79.1%) is good — it correctly identifies most people who don’t have Alzheimer’s.
This suggests your model is slightly better at ruling out Alzheimer’s (true negatives) than detecting 
it (true positives), which is common in imbalanced or subtle medical data.


The VIF (Variance Inflation Factor) results that we get above are excellent. What we see:

Variable	                      VIF	                Interpretation
Country	                        1.015	            No multicollinearity (great!)
Age	                            1.072	            Very low — no issues
Family History of Alzheimer's	  1.027	            Very low — safe
APOE ε4 Genetic Risk	          1.045	            Very low — safe
Urban vs Rural Living	          1.000	            Practically perfect

What do the above mean?
No multicollinearity at all in your final model.
This confirms that your predictors are statistically independent enough to trust the regression coefficients and p-values. We're good to go on interpretation, prediction, and evaluation.




--------
```{r  Balanced + Scaled dataset}
library(caret)

# Step 1: Rename the target variable to 'Group' (optional, for easier handling)
train_data$Group <- as.factor(train_data$Alzheimer.s.Diagnosis)

# Step 2: Upsample the data
set.seed(123)
train_upsampled <- upSample(x = train_data[, -which(names(train_data) %in% c("Group", "Alzheimer.s.Diagnosis"))],
                            y = train_data$Group)

# Step 3: Rename target column
names(train_upsampled)[ncol(train_upsampled)] <- "Group"

# Step 4: Scale the numeric predictors
pre_proc <- preProcess(train_upsampled[, -which(names(train_upsampled) == "Group")],
                       method = c("center", "scale"))

train_scaled <- predict(pre_proc, train_upsampled[, -which(names(train_upsampled) == "Group")])

# Step 5: Combine scaled predictors with target
scaled_upsampled_df <- cbind(train_scaled, Group = train_upsampled$Group)

# Step 6: Check the new dataset
str(scaled_upsampled_df)

scaled_upsampled_df
```

```{r  Split scaled_upsampled_df Dataset into Train/Test}
set.seed(123)
train_index <- createDataPartition(scaled_upsampled_df$Group, p = 0.8, list = FALSE)

train_set <- scaled_upsampled_df[train_index, ]
test_set  <- scaled_upsampled_df[-train_index, ]
```


```{r  Evaluation Function}
library(caret)
library(pROC)

evaluate_model <- function(model_name, predicted, actual, probs = NULL) {
  cm <- confusionMatrix(predicted, actual)
  acc <- cm$overall["Accuracy"]
  sens <- cm$byClass["Sensitivity"]
  spec <- cm$byClass["Specificity"]
  auc_val <- if (!is.null(probs)) {
    roc_obj <- roc(actual, as.numeric(probs), quiet = TRUE)
    auc(roc_obj)
  } else {
    NA
  }
  data.frame(Model = model_name,
             Accuracy = round(acc, 3),
             Sensitivity = round(sens, 3),
             Specificity = round(spec, 3),
             AUC = round(auc_val, 3))
}
```

```{r  Logistic, Decision Tree, Random Forest, SVM, KNN}
library(caret)
library(pROC)
library(rpart)
library(randomForest)
library(e1071)
library(class)
#  Train & evaluate all models
results <- list()

## 1. Logistic Regression
log_model <- glm(Group ~ ., data = train_set, family = "binomial")
log_probs <- predict(log_model, test_set, type = "response")
log_pred <- ifelse(log_probs > 0.5, "Yes", "No")
log_pred <- factor(log_pred, levels = levels(test_set$Group))
results[[1]] <- evaluate_model("Logistic Regression", log_pred, test_set$Group, log_probs)
```

```{r Decision Tree}
## 2. Decision Tree
dt_model <- rpart(Group ~ ., data = train_set, method = "class")
dt_pred <- predict(dt_model, test_set, type = "class")
dt_probs <- predict(dt_model, test_set)[, "Yes"]
results[[2]] <- evaluate_model("Decision Tree", dt_pred, test_set$Group, dt_probs)
```

```{r Random Forest}
## 3. Random Forest
rf_model <- randomForest(Group ~ ., data = train_set)
rf_pred <- predict(rf_model, test_set)
rf_probs <- predict(rf_model, test_set, type = "prob")[, "Yes"]
results[[3]] <- evaluate_model("Random Forest", rf_pred, test_set$Group, rf_probs)
```

```{r SVM}
## 4. Support Vector Machine (SVM)
svm_model <- svm(Group ~ ., data = train_set, probability = TRUE, kernel = "linear")
svm_pred <- predict(svm_model, test_set)
svm_probs <- attr(predict(svm_model, test_set, probability = TRUE), "probabilities")[, "Yes"]
results[[4]] <- evaluate_model("SVM", svm_pred, test_set$Group, svm_probs)
```



```{r KNN}
## 5. KNN
# Extract X and Y for KNN
train_x <- train_set[, -which(names(train_set) == "Group")]
test_x <- test_set[, -which(names(test_set) == "Group")]
train_y <- train_set$Group

# Run KNN with k = 5
knn_pred <- knn(train = train_x, test = test_x, cl = train_y, k = 5)
results[[5]] <- evaluate_model("KNN (k=5)", knn_pred, test_set$Group)
```

```{r results_df}

# Combine all results
results_df <- do.call(rbind, results)
print(results_df)
```

