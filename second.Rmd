---
title: "DSC 532- Fuel Regression project"
names: Eleni Yiasoumi , Marios Christodoulou , Iliana Frantzia
output:
  html_document:
    df_print: paged
date: "2025-04-01"
---

## Dataset Overview

The **FuelConsumption.csv** dataset contains information on 1,067 observations (vehicles from the model year 2014 only) and 13 columns (variables). Each row represents a different vehicle model and the variables describe both the technical specifications and environmental performance of the vehicles.

The columns of the dataset are :

- **MODELYEAR** :	The year the vehicle model was manufactured.
- **MAKE** :	The brand or manufacturer of the vehicle (e.g., Ford, BMW, Honda).
- **MODEL**	The specific model name of the vehicle.
- **VEHICLECLASS** :	Vehicle type  (e.g., SUV, Compact, Pickup truck).
- **ENGINESIZE** :	The engine size in liters (L) ,where a higher value typically means a larger engine.
- **CYLINDERS** :	The number of cylinders in the engine. More cylinders usually indicate higher power  and fuel consumption.
- **TRANSMISSION** :	The type of gearbox (e.g., automatic, manual).
- **FUELTYPE** :	The type of fuel the vehicle uses (e.g., gasoline, diesel, ethanol, natural gas).
- **FUELCONSUMPTION_CITY** :	Fuel consumption in city driving conditions (liters per 100 kilometers).
- **FUELCONSUMPTION_HWY** :	Fuel consumption on highways (liters per 100 kilometers).
- **FUELCONSUMPTION_COMB** :	Combined fuel consumption (weighted average of city and highway) in liters per 100 kilometers.
- **FUELCONSUMPTION_COMB_MPG** :	Combined fuel consumption in miles per gallon (MPG), this is the inverse of fuel consumption in L/100 km.
- **CO2EMISSIONS** :	The amount of carbon dioxide (CO₂) emitted by the vehicle’s tailpipe, measured in grams per kilometer (g/km). This is the target variable.


The primary objective of this project is to perform a regression analysis to explore the relationship between vehicle characteristics and their corresponding CO₂ emissions. By building a predictive model, we aim to understand which vehicle specifications most strongly influence CO₂ emissions, helping to identify key contributors to automotive environmental impact.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import Libraries

```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(moments)
library(nortest)
library(corrplot)
library(caret)
library(tidyr)
library(GGally)
library(car)
library(glmnet)
library(leaps)
library(pls)
```

Load the FuelConsumption dataset and preview the first 6 rows of the dataset.

```{r set_seed}
set.seed(123)
```

# Data Overview

```{r load-data}
df<- read.csv("FuelConsumption.csv")
head(df)
```

The dataset includes features like:

```{r columns}
colnames(df)
``` 
Data types and Structure of the dataset :

```{r structure}
str(df)
```

We have 1067 observations and 13 columns.


Statistical summary of numerical variables :

```{r summary}
summary(df)
```

- MODEL YEAR: All the observations in this column contain the year 2014 because every vehicle in the dataset was manufactured in 2014.We should remove this column for our dataset since it can’t help differentiate between outcomes.

- ENGINE_SIZE: Ranges from 1.0 to 8.4 liters and the mean engine size is 3.35 L, suggesting that most cars   have moderate engine sizes

- CYLINDERS: Ranges from 3 to 12 cylinders, with a median of 6 cylinders and the average number of  cylinders is around 5.8. In general,more cylinders imply more engine power and therefore more CO2Emissions.

- FUELCONSUMPTION_CITY / HWY / COMB: Combined fuel consumption (L/100 km) ranges from 4.7 to 25.8 and the   mean combined fuel consumption is 11.58, indicating a mix of fuel-efficient and less efficient vehicles.We are also going to check if there is a relationship between CMB and CITY,HWY.

- FUELCONSUMPTION_COMB_MPG: Ranges from 11 to 60 miles per gallon with median 26 MPG, meaning half the cars get more than 26 MPG and half get less.

- CO2EMISSIONS: Ranges from 108 to 488 grams/km, with a mean of 256.2 ,suggesting a wide spread in vehicle emissions, influenced by engine and fuel consumption differences

We also observed that the fuel consumption comb and the fuel consumption comb mpg possibly give us the same results but in different units so we will have to drop one of these columns to avoid multicollinearity and unnessary redundancy.

Also , vehicles with larger engines and more cylinders tend to have higher fuel consumption and CO₂ emissions.



```{r which}
# Model with the highest CO2 emissions
df$MODEL[which.max(df$CO2EMISSIONS)]

# Model with the lowest CO2 emissions
df$MODEL[which.min(df$CO2EMISSIONS)]
```

The vehicle model with the highest CO₂ emissions is the E350 WAGON, emitting ___ g/km, while the model with the lowest CO₂ emissions is the ____, emitting only ___ g/km.






Count the number **missing values** per column

```{r colsum}

colSums(is.na(df))  

```

We have 0 missing values, so we don’t need to impute or remove anything.


Number of duplicated rows:

```{r duplicate}

sum(duplicated(df))

```

There are no duplicated rows in the dataset.

Check unique values for categorical columns

```{r unique}

cat_cols <- names(df)[sapply(regression, is.character)]

unique_values <- lapply(df[cat_cols], unique)
unique_values

```

The dataset contains a rich variety of categorical features, including 39 car makes, hundreds of models, 16 vehicle classes, multiple transmission types and 4 fuel types.

All categorical columns are free of spelling errors, case inconsistencies, or data entry mistakes.

## Drop unessesary columns like Model year

```{r drop-MODELYEAR}
df <- df[, !(names(df) %in% "MODELYEAR")]
head(df)
```

## Difference between fuel consumption comb and mpg 

```{r mpg diff}

# 1. MPG Difference Check 
calculated_mpg <- 235.214583 / regression$FUELCONSUMPTION_COMB
mpg_diff <- regression$FUELCONSUMPTION_COMB_MPG - calculated_mpg

# Threshold for the top 5% largest differences
percentile_95 <- quantile(mpg_diff, 0.95)
print(percentile_95)

# Count how many exceed it
sum(mpg_diff > percentile_95)  # Still gives 52

```

We checked the difference between fuel consumption comb and mpg and we observed that these two variables express the same concept, fuel efficiency ,but in inverse units and the formula of the mpg calculation is :

MPG =235.214583 / (L/100km)

There are 52 outliers that have difference in calculations.

We will also remove FUELCONSUMPTION_COMB_MPG  from the dataset prior to analysis because it is mathematically derived from FUELCONSUMPTION_COMB using the above formula. Including both variables would introduce multicollinearity and redundancy in our regression model. 

## Drop the fuel consumption comb mpg column

```{r drop-MODELYEAR}
df <- df[, !(names(df) %in% "FUELCONSUMPTION_COMB_MPG")]
names(df)
```

# Fuel Consumption Check (Simple and Weighted Average)
```{r fuel-average}
# Simple average
simple_avg <- (df$FUELCONSUMPTION_CITY + df$FUELCONSUMPTION_HWY) / 2
# Weighted average
weighted_avg <- 0.55 * df$FUELCONSUMPTION_CITY + 0.45 * df$FUELCONSUMPTION_HWY

# Compare to actual
diff_simple <- abs(df$FUELCONSUMPTION_COMB - simple_avg)
diff_weighted <- abs(df$FUELCONSUMPTION_COMB - weighted_avg)

# Mean differences
mean(diff_simple)
mean(diff_weighted)

```

We first perform some tests to see which formula is used for the calculation of the average of fuel consumption in city and in highway.The two possible averages is the simple average and the 55/45 weighted average.By looking at the mean above, we can see that the formula with the lower mean difference is the one likely used to compute. 
So the formula used is:
     COMB = 0.55 x CITY + 0.45 x HWY

So there is a clearly linear realtionship between fuel comsumption combined with city and highway.
The next step is to set a threshold:
A proper threshold could be around 0.1 since the mean difference is close to 0.02510778


```{r relation-comb-city}

# Scatterplot: City vs Combined Fuel Consumption
ggplot(df, aes(x = FUELCONSUMPTION_CITY, y = FUELCONSUMPTION_COMB)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    title = "Linear Relationship Between City and Combined Fuel Consumption",
    x = "City Fuel Consumption (L/100 km)",
    y = "Combined Fuel Consumption (L/100 km)"
  )
```
```{r}
# Correlation
cor(df$FUELCONSUMPTION_CITY, df$FUELCONSUMPTION_COMB)

# Fit a linear model
lm_comb_city <- lm(FUELCONSUMPTION_COMB ~ FUELCONSUMPTION_CITY, data = df)
summary(lm_comb_city)

```

```{r}
# Scatterplot with linear regression line
ggplot(df, aes(x = FUELCONSUMPTION_CITY, y = FUELCONSUMPTION_HWY)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(
    title = "Linear Relationship Between City and Highway Fuel Consumption",
    x = "City Fuel Consumption (L/100 km)",
    y = "Highway Fuel Consumption (L/100 km)"
  )

# Correlation
cor(df$FUELCONSUMPTION_CITY, df$FUELCONSUMPTION_HWY)

# Linear model
model_fc <- lm(FUELCONSUMPTION_HWY ~ FUELCONSUMPTION_CITY, data = df)
summary(model_fc)

```



## Numerical columns plots


Histogram and boxplot of Engine Size distribution

```{r plots-engine}
par(mfrow = c(1, 2))
hist(df$ENGINESIZE,
     main = "Distribution of Engine Size",
     xlab = "Engine Size",
     col = "steelblue",
     border = "black")

boxplot(df$ENGINESIZE, 
        main = "ENGINE SIZE",
        ylab = "ENGINE SIZE",
        col = "lightblue")



```


Most vehicles have engine sizes between 2.0 and 4.0 liters, with notable peaks around 2.0L and 3.5L. The distribution is right-skewed, meaning that while smaller engines are very common, a smaller number of vehicles have larger engines (above 5.0L). This suggests that most vehicles in the dataset are built with fuel efficiency in mind, while larger engines are limited to performance.

```{r skew-engine}
skewness(df$ENGINESIZE)
```

The skewness is around 0.57 so we can comfirm that it is right-skewed as we observed from the histogram above.




Histogram of Cylinders distribution

```{r hist-cylinders}
ggplot(df, aes(x = CYLINDERS)) +
  geom_histogram(bins = 15, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Cylinders", x = "Cylinders", y = "Count")
```

The histogram shows the distribution of the number of cylinders across vehicles in the dataset. The majority of cars have 4, 6 or 8 cylinders, with 4-cylinder engines being the most common, followed by 6 and 8 cylinders. This aligns with typical engine configurations found in compact, mid-sized, and larger vehicles.Higher cylinder counts like 10 and 12 cylinders are rare, suggesting they are used in high-performance or luxury vehicles. Overall, the plot highlights that most vehicles are built for everyday efficiency, with fewer models designed for high power output.



Boxplot of Cylinders

```{r cylinders-boxplot}
boxplot(df$CYLINDERS, 
        main = "CYLINDERS",
        ylab = "Cylinders",
        col = "lightblue")

```

We do not observe any outliers in the cylinders distribution.


Histogram of Fuel Consumption City distribution
 
```{r hist-fuel-city}
ggplot(df, aes(x = FUELCONSUMPTION_CITY)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Fuel Consumption (City)", x = "Fuel Consumption City", y = "Count")
```

The histogram shows the distribution of fuel consumption in city driving conditions, measured in liters per 100 kilometers (L/100 km). Most vehicles consume between 8 and 15 L/100 km, with a clear peak around 11–13 L/100 km, suggesting that this is the typical range for city fuel efficiency in 2014 vehicles.
The distribution is right-skewed, showing that while most cars are moderately fuel-efficient in the city, there are several vehicles with much higher consumption, reaching up to 30 L/100 km. These higher values likely correspond to larger, heavier or performance vehicles.

```{r fuelconsCITY-boxplot}
boxplot(df$FUELCONSUMPTION_CITY, 
        main = "FUEL CONSUMPTION CITY",
        ylab = "FUEL CITY",
        col = "lightblue")

```

```{r skew-city}
skewness(df$FUELCONSUMPTION_CITY)

```

```{r skew-no-outliers-city}
skewness(regression_no_outliers$FUELCONSUMPTION_CITY)
```

As we can observe the skewness after removing outliers decreased to 0.52.


Histogram of Fuel Consumption Highway distribution

```{r fuel-hwy}
ggplot(df, aes(x = FUELCONSUMPTION_HWY)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Fuel Consumption (Highway)", x = "Fuel Consumption Hwy", y = "Count")
```

This histogram displays the distribution of fuel consumption during highway driving, measured in liters per 100 kilometers (L/100 km). Most vehicles consume between 7 and 11 L/100 km, with a clear peak around 8 to 9 L/100 km, suggesting that highway driving is generally more fuel-efficient compared to city driving.Similar to the city fuel consumption plot, the distribution is right-skewed, with a smaller number of vehicles showing much higher highway fuel use, exceeding 15 L/100 km. These outliers likely correspond to larger or high-performance vehicles that are less optimized for long-distance efficiency.

```{r fuelconsHWY-box}
boxplot(df$FUELCONSUMPTION_HWY, 
        main = "FUEL CONSUMPTION HIGHWAY",
        ylab = "hwy",
        col = "lightblue")

```

Histogram of Fuel Consumption Combined distribution

```{r hist-fuel-comb}
ggplot(df, aes(x = FUELCONSUMPTION_COMB)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Fuel Consumption (Combined)", x = "Fuel Consumption Comb", y = "Count")
```

This histogram shows the distribution of combined fuel consumption, which averages city and highway fuel use, measured in liters per 100 kilometers (L/100 km). The majority of vehicles fall between 8 and 14 L/100 km, with a peak around 10 to 11 L/100 km, indicating that most cars in the dataset are moderately fuel-efficient under average driving conditions.As from  previous, the distribution is right-skewed, meaning a small number of vehicles consume significantly more fuel,up to over 25 L/100 km. These high-consumption outliers likely represent large SUVs, trucks or performance vehicles.



Boxplot of fuel consumption combined

```{r fuelconsCOMB-boxplot}
boxplot(df$FUELCONSUMPTION_COMB, 
        main = "FUEL CONSUMPTION COMBINED",
        ylab = "comb",
        col = "lightblue")

```





Histogram of Fuel Consumption MPG distribution

```{r hist-fuel-mpg}
ggplot(df, aes(x = FUELCONSUMPTION_COMB_MPG)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Fuel Consumption (MPG)", x = "Fuel Consumption Comb MPG", y = "Count")
```

This histogram displays the combined fuel consumption in miles per gallon (MPG), which is a commonly used measure of vehicle fuel efficiency. Higher MPG values indicate better fuel economy. Most vehicles in the dataset fall between 15 and 35 MPG, with a peak around 25 to 30 MPG.The distribution is left-skewed, since a higher MPG is better and fewer vehicles reach the high-efficiency range above 40 MPG. 


Boxplot of fuel consumption combined mpg:

```{r fuelconsHWY-boxplot}
boxplot(df$FUELCONSUMPTION_COMB_MPG, 
        main = "FUEL CONSUMPTION COMBINED MPG",
        ylab = "comb mpg",
        col = "lightblue")

```



Histogram of CO2 Emissions distribution
 
```{r hist-co2}
ggplot(df, aes(x = CO2EMISSIONS)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution of CO2 Emissions", x = "CO2 Emissions", y = "Count")
```

The histogram illustrates the distribution of CO₂ emissions (in grams per kilometer) for cars. Most cars emit between 180 and 300 g/km, with the highest concentration around 200–250 g/km, which likely represents average passenger vehicles.

The distribution shows a slight right skew, indicating that while most vehicles have moderate emissions, there are several high-emission outliers exceeding 400 g/km. These are likely large-engine, high-consumption models such as SUVs or performance vehicles.

To comfirm the skewness futher

```{r skew-co2}
skewness(df$CO2EMISSIONS)

```

As we can observe the skewness is >0.5 comfirming that the distribution is slightly right skewed.

```{r boxplot co2}
boxplot(df$CO2EMISSIONS, 
        main = "CO2 EMISSIONS",
        ylab = "CO2 EMISSIONS",
        col = "lightblue")


```

CO2EMISSIONS: Most values are concentrated between 150 and 300 g/km, with a gradual decline toward higher emissions. This suggests that have a slightly skewed distribution.

CYLINDERS: The distribution is clearly categorical, with strong peaks at 4, 6, and 8 cylinders—indicating these are the dominant engine types in the dataset.

ENGINESIZE: Smaller engines (2 to 3.5 liters) are more common, while larger engines appear less frequently. The data is not perfectly symmetrical, showing some right skew.

FUELCONSUMPTION_COMB: The majority of vehicles consume around 8 to 12 liters per 100 km. Fewer vehicles fall into the higher consumption range, again showing a right-skewed shape.



We are going to create a new dataframe without outliers just so we can compare the models on the two dataframes.

```{r create-new-dataframe-no-outliers}
# Step 1: Select only numeric columns
numeric_cols <- sapply(df, is.numeric)

# Step 2: Define a function to remove outliers based on IQR
remove_outliers <- function(df) {
  for (col in names(df)) {
    if (is.numeric(df[[col]])) {
      Q1 <- quantile(df[[col]], 0.25, na.rm = TRUE)
      Q3 <- quantile(df[[col]], 0.75, na.rm = TRUE)
      IQR_value <- Q3 - Q1
      lower_bound <- Q1 - 1.5 * IQR_value
      upper_bound <- Q3 + 1.5 * IQR_value
      
      df <- df[df[[col]] >= lower_bound & df[[col]] <= upper_bound, ]
    }
  }
  return(df)
}

# Step 3: Apply the function to your dataset
regression_no_outliers <- remove_outliers(df)
```

## Categorical columns plots 

Barplot of make distribution:

```{r, fig.width=10, fig.height=12}
df %>%
  count(MAKE, sort = TRUE) %>%
  ggplot(aes(x = reorder(MAKE, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Car Makes", x = "Make", y = "Count")
```


From the plot, we observe that the most common car makes in the dataset are Ford, Chevrolet, BMW, Mercedes-Benz and Toyota.On the other end, premium and niche brands like Rolls-Royce, Aston Martin and Lamborghini appear far less frequently, likely due to their lower production volumes.



Barplot of model disribution:

```{r bar-model}
df %>%
  count(MODEL, sort = TRUE) %>%
  top_n(10) %>%
  ggplot(aes(x = reorder(MODEL, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 10 Car Models", x = "Model", y = "Count")
```

The bar chart above displays the top 10 most frequently occurring car models in the dataset. The most common models are:F150 FFV 4X4,F150 FFV,FOCUS FFV,BEETLE,ACCORD.These models appear between 5 and 8 times each in the dataset.Notably, Ford has multiple variants of the F150 among the most frequent models, reflecting its strong market presence and product variety in 2014.


Barplot of class distribution: 

```{r bar-class}


ggplot(df, aes(x = fct_rev(fct_infreq(VEHICLECLASS)), fill = VEHICLECLASS)) +
  geom_bar(fill = "skyblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Distribution of Vehicle Classes", x = "Vehicle Class", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")


```



The bar chart above presents the distribution of vehicle classes within the dataset. The most common classes are mid-size, compact, small SUVs and standard SUVs, followed by full-size vehicles. Smaller segments of the dataset include two-seaters, subcompacts and pickup trucks, as well as a variety of less common vehicle types such as minivans,vans, station wagons and special purpose vehicles.


Barplot of fuel type distribution:

```{r bar-fueltype}

ggplot(df, aes(x = FUELTYPE, fill = FUELTYPE)) +
  geom_bar(fill="skyblue") +
  theme_minimal() +
  labs(title = "Fuel Type Distribution", x = "Fuel Type", y = "Count") +
  theme(legend.position = "none")
```

The fuel type distribution chart reveals that the majority of vehicles in the dataset run on regular gasoline (X), followed closely by those using premium gasoline (Z). In contrast, vehicles that use ethanol (E85) and diesel (D) are significantly less common. This suggests that the dataset is heavily weighted toward traditional gasoline-powered vehicles, which reflects market trends in 2014.It is important to consider this imbalance when analyzing fuel consumption and CO₂ emissions, as different fuel types have distinct environmental and performance characteristics.





```{r bar-transmission}
ggplot(df, aes(x = fct_rev(fct_infreq(TRANSMISSION)), fill = VEHICLECLASS)) +
  geom_bar(fill = "skyblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Distribution of Transmission Code", x = "Transmission Code", y = "Count") 


```

The bar chart shows the distribution of transmission types in the dataset, represented by coded labels. The most common transmission types are A6 (6-speed automatic), AS6 (6-speed automatic with manual mode), and M6 (6-speed manual), indicating that 6-speed systems dominate among the recorded vehicles. Other frequently appearing codes include A8 and AS8, which reflect more modern 8-speed automatics.However, it is clear that automatic transmissions are more prevalent.This variation in transmission types may affect fuel efficiency and CO₂ emissions, as different transmission technologies can influence engine performance and vehicle efficiency. 









We can also check by using the IQR method:
```{r IQR method for threshold}
# Use IQR method for threshold
Q1 <- quantile(diff_weighted, 0.25)
Q3 <- quantile(diff_weighted, 0.75)
IQR <- Q3 - Q1
threshold <- Q3 + 1.5 * IQR
print(threshold)

# Check how many values are above threshold
sum(diff_weighted > threshold)

# Or use 0.1 as your cutoff
sum(diff_weighted > 0.1)
```

Since also with 0.1 threshold we can see that there are no points with possible noise, we assume that all the calculations are done correctly.

## Create a new column- Transmission Group

```{r NA}
df$TransmissionGroup <- NA
```

Add all transmission groupings

```{r convert-factor}
df$TransmissionGroup[grepl("^A[0-9]$", regression$TRANSMISSION)] <- "Automatic"
df$TransmissionGroup[grepl("^AM[0-9]$", regression$TRANSMISSION)] <- "Automated Manual"
df$TransmissionGroup[grepl("^AS[0-9]$", regression$TRANSMISSION)] <- "Select Shift Auto"
df$TransmissionGroup[grepl("^AV", regression$TRANSMISSION)] <- "CVT"
df$TransmissionGroup[grepl("^M[0-9]$", regression$TRANSMISSION)] <- "Manual"  # NEW

# Convert to factor
df$TransmissionGroup <- factor(df$TransmissionGroup)

# Check if any values still NA
table(df$TransmissionGroup, useNA = "ifany")

head(df)

sum(is.na(df$TransmissionGroup))
```

### Plots compared with CO2EMISSIONS

```{r trans-ggplot}
ggplot(df %>% filter(!is.na(TransmissionGroup)), aes(x = TransmissionGroup, y = CO2EMISSIONS, fill = TransmissionGroup)) +
  geom_boxplot() +
  theme_minimal()

```

```{r table-trans}
table(df$TransmissionGroup, useNA = "ifany")
sum(is.na(df$TransmissionGroup))
```
The boxplot illustrates the distribution of CO₂ emissions across different transmission types, grouped into five categories: Automated Manual, Automatic, CVT (Continuously Variable Transmission), Manual, and Select Shift Automatic. From the visualization, we observe that automatic transmissions have the highest variability in emissions and among the highest median values, indicating that these vehicles tend to emit more CO₂ on average. Similarly, select shift automatics and automated manuals also show relatively high median emissions and wider interquartile ranges, suggesting that they are often found in higher-emission vehicles.

In contrast, CVT-equipped vehicles have the lowest median CO₂ emissions, with a compact distribution and fewer high-emission outliers. This reflects the fuel-efficient nature of CVT technology, commonly used in smaller or economy-focused vehicles. Manual transmissions also show a lower median compared to automatics, though with a broader spread of values.

Overall, the plot highlights that transmission type is a relevant factor influencing CO₂ emissions. Vehicles with CVTs and manual transmissions tend to be more environmentally friendly in terms of emissions, while automatics and select shift systems are generally associated with higher emissions, likely due to being paired with more powerful engines or heavier vehicle classes.


```{r scatter-enginesize}
# Scatterplots with trend lines
ggplot(df, aes(x = ENGINESIZE, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() +
  labs(title = "CO2 Emissions vs Engine Size", x = "Engine Size (L)", y = "CO2 Emissions (g/km)")
```

There is a clear positive relationship. As fuel consumption increases, CO2 emissions also increase. The points are close to the line, showing a strong connection.



```{r ggplot-comb}
ggplot(df, aes(x = FUELCONSUMPTION_COMB, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "darkgreen") +
  theme_minimal() +
  labs(title = "CO2 Emissions vs Combined Fuel Consumption")
```

This plot also shows a positive trend. Vehicles with bigger engines usually produce more CO2, but the points are more spread out, so the relationship is not as strong as with fuel consumption.




```{r ggplot-fueltype}
ggplot(df, aes(x = FUELTYPE, y = CO2EMISSIONS, fill = FUELTYPE)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "CO2 Emissions by Fuel Type", x = "Fuel Type", y = "CO2 Emissions (g/km)") +
  theme(legend.position = "none")  

```

CO2 Emissions by Fuel Type:

Vehicles using fuel type E tend to have the highest CO2 emissions based on the median.
Fuel type D shows the lowest median emissions.
All fuel types show some variation, but type E also has a wider range and more high outliers.



### Correlations with C02 Emissions

```{r corr-co2}
numeric_data <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_target <- cor_matrix[, "CO2EMISSIONS"]
sort(cor_target, decreasing = TRUE)

```

Scatterplot matrix of all numerical features

```{r pairs-numeric}


# Create scatterplot matrix
pairs(numeric_data, main = "Scatterplot Matrix of All Numeric Variables")

```

There is a strong positive correlation between:

ENGINESIZE and CO2EMISSIONS
CYLINDERS and CO2EMISSIONS
FUELCONSUMPTION_COMB and CO2EMISSIONS
FUELCONSUMPTION_CITY and FUELCONSUMPTION_HWY
FUELCONSUMPTION_COMB and both CITY and HWY consumption

There is also a strong negative correlation between:

FUELCONSUMPTION_COMB_MPG and CO2EMISSIONS, as well as other fuel consumption variables
(This makes sense: higher MPG means lower fuel use → lower emissions)
Some variables like MODELYEAR show no variation (constant in 2014), and therefore do not contribute useful information in a regression context.

This matrix supports the inclusion of variables like ENGINESIZE, FUELCONSUMPTION_COMB, and CYLINDERS as strong predictors in the regression model. It also raises the possibility of multicollinearity among fuel consumption variables, which should be investigated further using tools like the Variance Inflation Factor (VIF).




Scatterplot matrux of selected features

```{r pairs-plot-numeric}
pairs(df[, c("ENGINESIZE", "CYLINDERS", "FUELCONSUMPTION_COMB", "CO2EMISSIONS")],
      main = "Scatterplot Matrix of Selected Variables")
```

The scatterplot matrix shows that fuel consumption, engine size, and number of cylinders all have a positive relationship with CO₂ emissions. This means that cars that use more fuel, have bigger engines, or more cylinders tend to produce more CO₂. Among these, fuel consumption has the strongest relationship with CO₂ emissions. The plot also shows that these three variables are related to each other, which could cause issues in a regression model. This suggests we should check for multicollinearity before using them all together in one model.







```{r numeric-columns}
# Only numeric columns
cor_data <- df %>% select_if(is.numeric)
cor_matrix <- cor(cor_data)

# Plot
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.8, title = "Correlation Matrix", mar=c(0,0,1,0))
```



Dark blue = strong positive correlation (close to +1)
Dark red = strong negative correlation (close to -1)
Lighter colors = weak or no correlation (close to 0)

CO2EMISSIONS is strongly linked to:
FUELCONSUMPTION_COMB
FUELCONSUMPTION_CITY
FUELCONSUMPTION_HWY
ENGINESIZE and CYLINDERS

FUELCONSUMPTION_COMB_MPG is negatively related to CO2 (more MPG = less emissions).

MODELYEAR has no clear connection with the other variables.


### Scatterplots with target variable

```{r scatter-engine}
ggplot(df, aes(x = ENGINESIZE, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Engine Size vs CO2 Emissions")
```

```{r scatter-cylinders}
ggplot(df, aes(x = CYLINDERS, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Cylinders vs CO2 Emissions")
```

```{r scatter-fuel-city}
ggplot(df, aes(x = FUELCONSUMPTION_CITY, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Fuel consumption city vs CO2 Emissions")
```


```{r scatter-fuel-hwy}
ggplot(df, aes(x = FUELCONSUMPTION_HWY, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Fuel consumption highway vs CO2 Emissions")
```

```{r scatter-fuel-cmb}
ggplot(df, aes(x = FUELCONSUMPTION_COMB, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Fuel consumption combined vs CO2 Emissions")
```


### Logarithmic Transformation and Scale to numerical features

```{r log-numerical}
numeric_columns <- sapply(df, is.numeric)

df_log_scaled <- df  
df_log_scaled[numeric_columns] <- lapply(df[numeric_columns], log1p)
df_log_scaled[numeric_columns] <- scale(df_log_scaled[numeric_columns])
head(df_log_scaled)
summary(df_log_scaled[numeric_columns])  # Means ≈ 0, Std Dev ≈ 1

```

All the numeric columns that have been log-transformed and standardized (mean 0, std dev 1)

```{r}
model_c_h<-lm(CO2EMISSIONS~FUELCONSUMPTION_CITY+FUELCONSUMPTION_HWY,data=train)
summary(model_c_h)
```

### Scale numerical features

```{r scale}


# Fit the regression model
model_scaled <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_CITY + 
                   FUELCONSUMPTION_HWY + FUELCONSUMPTION_COMB + FUELCONSUMPTION_COMB_MPG,
                   data = fuel_scaled)

# View standardized coefficients
summary(model_scaled)
```



We are going to split our data to train and test sets (80% train data and 20% test data)

```{r split}

split <- createDataPartition(df$CO2EMISSIONS, p = 0.8, list = FALSE)
train <- df[split, ]
test <- df[-split, ]
```

```{r train-model-make}
train$MAKE <- as.character(train$MAKE)
train$MODEL <- as.character(train$MODEL)
```

```{r split-scaled}
split <- createDataPartition(df_log_scaled$CO2EMISSIONS, p = 0.8, list = FALSE)

# Apply the split to the processed dataset
train_log_scaled <- df_log_scaled[split, ]
test_log_scaled  <- df_log_scaled[-split, ]
```

## Model full all

```{r model-full-all-1}
model_full_all<-lm(CO2EMISSIONS ~ ., data = train)
summary(model_full_all)
```


## Model full all with log and scale

```{r model-scaled-log}
model_scaled_log <- lm(CO2EMISSIONS ~.,
                 data = train_log_scaled)
summary(model_scaled_log)
```






### Linear Regression Models


```{r model full}
model_full <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_CITY +
                   FUELCONSUMPTION_HWY + FUELCONSUMPTION_COMB + FUELCONSUMPTION_COMB_MPG,
                 data = train)
summary(model_full)
``` 
 
A full linear regression model was built to predict CO₂ emissions using engine size, number of cylinders, and several fuel consumption measures. The model performed strongly with an R² of 88.2%, indicating it explains a large portion of the variability in emissions. Engine size, number of cylinders, and fuel efficiency (measured in miles per gallon) were statistically significant predictors. However, the fuel consumption variables in liters/100km were not significant, likely due to multicollinearity. This suggests that some variables may be redundant and could be removed to improve model clarity and reduce multicollinearity.


Coefficients of your full linear regression model:

```{r}
Yhat_full= model_full$coefficients
Yhat_full

```

Check for Multicollinearity:

```{r vif-full}
vif(model_full)

```

The full model showed extreme multicollinearity, especially among the fuel consumption variables (city,highway,combined), with VIF values exceeding 14,000 , this means that these variables are highly correlated with each other and keeping them all causes instability in our model. 


A simplified model using only key variables (engine size, number of cylinders and combined fuel consumption) to improve model stability and interpretability

```{r model-multi}
model_multi <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB , data = train)
summary(model_multi)
```


The multiple linear regression model was created to predict CO₂ emissions using engine size, number of cylinders and combined fuel consumption. The model worked very well, explaining about 86% (R² = 0.859) of the variation in CO₂ emissions. All three variables were statistically significant (p<0.001), meaning they each have a strong effect. The results showed that cars with bigger engines, more cylinders and higher fuel consumption produce more CO₂. This model is simpler and easier to understand than the full model, but still very accurate.




```{r vif-multi}
vif(model_multi)
```

The new model shows VIF values below 10 for all predictors, confirming that multicollinearity has been effectively reduced. This improves the reliability of the coefficient estimates and supports the use of this simpler model for interpretation and prediction.

```{r model-hwy-city}
model_city_hwy <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + 
                     FUELCONSUMPTION_CITY + FUELCONSUMPTION_HWY, data = train)

summary(model_city_hwy)
```

The multiple linear regression model was developed to predict CO₂ emissions using engine size, number of cylinders, fuel consumption in the city, and fuel consumption on the highway. The model performed well, explaining about 86% of the variation in CO₂ emissions (R² = 0.859). All four variables were statistically significant, meaning they each have a strong impact on emissions. The results showed that vehicles with larger engines, more cylinders, and higher fuel consumption in both city and highway driving tend to produce more CO₂. This model highlights how both types of driving conditions contribute to emissions.


```{r vif-city-hwy}
vif(model_city_hwy)

```

The model using both city and highway fuel consumption produced high VIF values (above 20), indicating strong multicollinearity between these variables.So the best model with the lowest multicollinearity is the one with fuel consumption combined.



Simple linear models 

```{r linear-model}
# Fit the model
model_simple <- lm(CO2EMISSIONS ~ FUELCONSUMPTION_COMB, data = train)

# See the summary
summary(model_simple)
```

```{r ggplot-fuelconsumption}
ggplot(train, aes(x = FUELCONSUMPTION_COMB, y = CO2EMISSIONS)) +
  geom_point(color = "black", alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Simple Linear Regression: CO2 vs Fuel Consumption",
       x = "Combined Fuel Consumption (L/100km)",
       y = "CO2 Emissions (g/km)") +
  theme_minimal()
```

```{r model-engine}
# Simple linear regression: CO2 vs Engine Size
model_engine <- lm(CO2EMISSIONS ~ ENGINESIZE, data = train)

# Show the summary
summary(model_engine)
```
The linear model:

CO2EMISSIONS =125.30+39.13×ENGINESIZE

The null hyppothesis (H0) is that engine size has no effect on CO2 emissions (H0: β1 = 0)

The alternative hypothesis(H1) is that engine size does have an effect on CO2 emissions. (H1: β1 != 0)

The p-value for the ENGINESIZE coefficient is < 2e-16, which is much smaller than the significance level of 0.05.
Therefore, we reject the null hypothesis and conclude that engine size has a statistically significant effect on CO2 emissions.


Model Fit:

R-squared = 0.7641 ,where ~76.4% of the variability in CO2 emissions is explained by engine size alone This is quite strong for a single predictor
Adjusted R-squared = 0.7639 .Almost the same, which confirms the model isn’t overfitting.




```{r ggplot co2-engine}
ggplot(train, aes(x = ENGINESIZE, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Simple Linear Regression: CO2 vs Engine Size",
       x = "Engine Size (L)",
       y = "CO2 Emissions (g/km)") +
  theme_minimal()
```



Both variables are strong predictors of CO2 emissions.
The model with FUELCONSUMPTION_COMB explains slightly more of the variation (R² = 0.796 vs 0.764).
Residuals are smaller in the first model, which also suggests a better fit.
Both models are statistically significant and show clear positive trends.
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.




Model Evaluation 


```{r coef-model}
coef(model_full)
```

```{r predict-full}
# Predict on test data
predicted_test <- predict(model_full, newdata = test)

# Add predictions to test set
test$predicted_CO2 <- predicted_test


# Calculate Mean Absolute Error
mae_full <- mean(abs(test$CO2EMISSIONS - test$predicted_CO2))

# Root Mean Squared Error
rmse_full <- sqrt(mean((test$CO2EMISSIONS - test$predicted_CO2)^2))

# Print results
mae_full
rmse_full

```

After training the full linear regression model, predictions were made on the test set to evaluate its performance. The Mean Absolute Error (MAE) was 12.7 g/km, meaning that, on average, the model's predictions were about 12.7 grams of CO₂ per kilometer off from the actual values. The Root Mean Squared Error (RMSE) was 18.5 g/km, which gives more weight to larger errors. These results indicate that the model makes fairly accurate predictions and performs well at estimating vehicle CO₂ emissions.




Plot actual vs predicted CO2 Emissions

```{r plot-actual-pred-co2}

ggplot(test, aes(x = CO2EMISSIONS, y = predicted_CO2)) +
  geom_point(color = "steelblue", alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(
    title = "Actual vs Predicted CO2 Emissions",
    x = "Actual CO2 Emissions (g/km)",
    y = "Predicted CO2 Emissions (g/km)"
  ) +
  theme_minimal()
```

The scatterplot shows the relationship between the actual and predicted CO2 emissions for the vehicles in the test set. The red line represents perfect predictions (where predicted = actual). Most blue points are close to the red line, showing that the model makes performs well, with accurate predictions for a wide range of emission values.A few points deviate from the line, especially at higher emission levels, suggesting that the model  underestimates or overestimates emissions for some vehicles. However, the overall trend is strong and consistent, confirming that the model captures the relationship between the predictors and CO2 emissions effectively.




```{r modelfull-plot}
par(mfrow = c(2, 2))
plot(model_full)
```




```{r model-final}
model_final <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB , data = train)
predictions <- predict(model_final, test)

# RMSE
rmse <- sqrt(mean((predictions - test$CO2EMISSIONS)^2))
rmse
```

We evaluate the model by splitting the data into training and test sets (80/20). We then compare actual vs predicted values and inspect residuals.

```{r actual-predict}
# Plot: Actual vs Predicted
ggplot(data.frame(Actual = test$CO2EMISSIONS, Predicted = predictions),
       aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "blue") +
  theme_minimal() +
  labs(title = "Actual vs Predicted CO2 Emissions",
       x = "Actual CO2",
       y = "Predicted CO2")
```




```{r residuals}
residuals <- test$CO2EMISSIONS - predictions

ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  theme_minimal() +
  labs(title = "Distribution of Residuals",
       x = "Residuals (Actual - Predicted)",
       y = "Count")


```


```{r dataframe-res}
ggplot(data.frame(Residuals = residuals), aes(x = "", y = Residuals)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Boxplot of Residuals",
       x = "",
       y = "Residuals (Actual - Predicted)")

```


Now we are going to perform the models we did above for the dataset with no outliers.

We are going to split our data again to train and test sets.
```{r split-no-outliers}

split_no_outliers <- createDataPartition(regression_no_outliers$CO2EMISSIONS, p = 0.8, list = FALSE)
train_no_outliers <- regression_no_outliers[split, ]
test_no_outliers <- regression_no_outliers[-split, ]
```

Simple Linear Regression
```{r linear-model-no-outliers}
# Fit the model
model_simple_no_outliers <- lm(CO2EMISSIONS ~ FUELCONSUMPTION_COMB, data = train_no_outliers)

# See the summary
summary(model_simple_no_outliers)
```

```{r ggplot-fuelconsumption-no-outliers}
ggplot(train_no_outliers, aes(x = FUELCONSUMPTION_COMB, y = CO2EMISSIONS)) +
  geom_point(color = "black", alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Simple Linear Regression: CO2 vs Fuel Consumption",
       x = "Combined Fuel Consumption (L/100km)",
       y = "CO2 Emissions (g/km)") +
  theme_minimal()
```

```{r model-engine-no-outliers}
# Simple linear regression: CO2 vs Engine Size
model_engine_no_outliers <- lm(CO2EMISSIONS ~ ENGINESIZE, data = train_no_outliers)

# Show the summary
summary(model_engine_no_outliers)
```




```{r ggplot co2-engine-no-outliers}
ggplot(train_no_outliers, aes(x = ENGINESIZE, y = CO2EMISSIONS)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Simple Linear Regression: CO2 vs Engine Size",
       x = "Engine Size (L)",
       y = "CO2 Emissions (g/km)") +
  theme_minimal()
```



Model Evaluation 

Full model

```{r model full-no-outliers}
model_full_no_outliers <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_CITY +
                   FUELCONSUMPTION_HWY + FUELCONSUMPTION_COMB + FUELCONSUMPTION_COMB_MPG,
                 data = train_no_outliers)
summary(model_full_no_outliers)
```
```{r name-model-no-outliers}
names(model_full_no_outliers)
```

```{r coef-model-no-outliers}
coef(model_full_no_outliers)
```
```{r predict-full-no-outliers}
predict(model_full_no_outliers, 
        data.frame(
          ENGINESIZE = c(2.0, 3.0, 4.0),        # variable you're changing
          CYLINDERS = 6,
          FUELCONSUMPTION_CITY = 12.0,
          FUELCONSUMPTION_HWY = 8.5,
          FUELCONSUMPTION_COMB = 10.2,
          FUELCONSUMPTION_COMB_MPG = 27
        ), 
        interval = "confidence")

```


```{r modelfull-plot-no-outliers}
par(mfrow = c(2, 2))
plot(model_full_no_outliers)
```



Let’s use a few relevant predictors from your correlation matrix:
ENGINESIZE, CYLINDERS, FUELCONSUMPTION_COMB

```{r model-multi-no-outliers}
model_multi_no_outliers <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB , data = train_no_outliers)
summary(model_multi_no_outliers)
```



```{r model-final-no-outliers}
model_final_no_outliers <- lm(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB , data = train_no_outliers)
predictions_no_outliers <- predict(model_final_no_outliers, newdata = test_no_outliers)


# RMSE
rmse_no_outliers <- sqrt(mean((predictions_no_outliers - test_no_outliers$CO2EMISSIONS)^2))
rmse_no_outliers
```

```{r actual-predict-no-outliers}
ggplot(data.frame(Actual = test_no_outliers$CO2EMISSIONS, 
                  Predicted = predictions_no_outliers),
       aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "blue") +
  theme_minimal() +
  labs(title = "Actual vs Predicted CO₂ Emissions",
       x = "Actual CO₂",
       y = "Predicted CO₂")
```

```{r residuals-no-outliers}
length(test_no_outliers$CO2EMISSIONS)
length(predictions_no_outliers)
```



```{r residuals-no-outliers-2}
residuals_no_outliers <- test_no_outliers$CO2EMISSIONS - predictions_no_outliers

ggplot(data.frame(Residuals = residuals_no_outliers), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  theme_minimal() +
  labs(title = "Distribution of Residuals",
       x = "Residuals (Actual - Predicted)",
       y = "Count")


```


```{r dataframe-res-no-outliers}
ggplot(data.frame(Residuals = residuals_no_outliers), aes(x = "", y = Residuals)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Boxplot of Residuals",
       x = "",
       y = "Residuals (Actual - Predicted)")

```


## COMPARE RESULTS

```{r compare}
get_model_metrics <- function(model, test_data, target = "CO2EMISSIONS") {
  predictions <- predict(model, newdata = test_data)
  actual <- test_data[[target]]
  
  rmse <- sqrt(mean((predictions - actual)^2))
  r_squared <- summary(model)$r.squared
  
  data.frame(
    Model = deparse(formula(model)[[3]]),
    R_squared = round(r_squared, 4),
    RMSE = round(rmse, 4)
  )
}

results_no_outliers <- rbind(
  cbind(Dataset = "No Outliers", get_model_metrics(model_simple_no_outliers, test_no_outliers)),
  cbind(Dataset = "No Outliers", get_model_metrics(model_engine_no_outliers, test_no_outliers)),
  cbind(Dataset = "No Outliers", get_model_metrics(model_full_no_outliers, test_no_outliers)),
  cbind(Dataset = "No Outliers", get_model_metrics(model_multi_no_outliers, test_no_outliers)),
  cbind(Dataset = "No Outliers", get_model_metrics(model_final_no_outliers, test_no_outliers))
)

results_with_outliers <- rbind(
  cbind(Dataset = "With Outliers", get_model_metrics(model_simple, test)),
  cbind(Dataset = "With Outliers", get_model_metrics(model_engine, test)),
  cbind(Dataset = "With Outliers", get_model_metrics(model_full, test)),
  cbind(Dataset = "With Outliers", get_model_metrics(model_multi, test)),
  cbind(Dataset = "With Outliers", get_model_metrics(model_final, test))
)


comparison_table <- rbind(results_with_outliers, results_no_outliers)

# Display using knitr::kable (if in R Markdown)
knitr::kable(comparison_table, caption = "Model Performance Comparison (With vs Without Outliers)")


```

Conclusion

Which variables were most important?
The most important variable in predicting CO2 emissions was FUELCONSUMPTION_COMB, which had the highest correlation and produced the strongest simple regression model. Other useful predictors included ENGINESIZE, CYLINDERS, and TransmissionType.

Was the model good?
Yes, the final multiple regression model performed well. It explained a large part of the variation in CO2 emissions (high R² value), and its predictions were close to the actual values in the test set. The residuals were centered around zero with a few expected outliers, showing the model was reasonably accurate.

Non-linear transformations

```{r nonlinear}
model_poly <- lm(CO2EMISSIONS ~ ENGINESIZE + I(ENGINESIZE^2) + CYLINDERS + FUELCONSUMPTION_COMB, data = train)
summary(model_poly)
```

```{r model-log}
model_log <- lm(CO2EMISSIONS ~ log(ENGINESIZE) + CYLINDERS + FUELCONSUMPTION_COMB, data = train)
summary(model_log)
```

```{r model-interact}
model_interact <- lm(CO2EMISSIONS ~ ENGINESIZE * CYLINDERS + FUELCONSUMPTION_COMB, data = train)
summary(model_interact)
```


```{r anova}
anova(model_multi,model_poly)
```

Qualitative Predictors

```{r model-categorical}
categoricals <- c("MAKE", "MODEL", "VEHICLECLASS", "TRANSMISSION", "FUELTYPE")
train[categoricals] <- lapply(train[categoricals], as.factor)
model_all_cat <- lm(CO2EMISSIONS ~ VEHICLECLASS + TRANSMISSION + FUELTYPE +
                      ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB,
                    data = train)

summary(model_all_cat)
```

A multiple linear regression model was fitted using both numeric and categorical variables to predict CO₂ emissions. The model included engine size, number of cylinders, combined fuel consumption, vehicle class, transmission type, and fuel type. The model performed extremely well, explaining 99% of the variation in CO₂ emissions (R² = 0.9904). Combined fuel consumption was the strongest predictor, with higher fuel consumption leading to higher emissions. Some categorical variables such as certain transmission types and fuel types were also statistically significant. Vehicles using alternative fuel types (e.g., ethanol or electric) were associated with significantly lower CO₂ emissions. Overall, the model confirms that both vehicle characteristics and fuel types are important drivers of emissions.



```{r predict-all-cat}
# Generate predicted values
test$predicted_CO2 <- predict(model_all_cat, newdata = test)
# Calculate MAE and RMSE
mae_all_cat <- mean(abs(test$CO2EMISSIONS - test$predicted_CO2))
rmse_all_cat <- sqrt(mean((test$CO2EMISSIONS - test$predicted_CO2)^2))

# Print the results
mae_all_cat
rmse_all_cat
```

These low values indicate that the model provides highly accurate predictions of CO₂ emissions. The performance reflects the strength of the model's structure, which includes key vehicle specifications and categorical attributes such as transmission type and fuel type.

```{r plot-all-cat}
ggplot(test, aes(x = CO2EMISSIONS, y = predicted_CO2)) +
  geom_point(color = "steelblue", alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted CO2 Emissions",
    x = "Actual CO2 Emissions (g/km)",
    y = "Predicted CO2 Emissions (g/km)"
  ) +
  theme_minimal()
```

The above are comfirm visually ,we can observe a strong linear relationship between the actual and predicted CO₂ emissions on the test data. Most of the points lie very close to the red  line, which represents perfect predictions. This indicates that the model is making highly accurate predictions across a wide range of emission values. There are only a few minor deviations, suggesting excellent generalization and model fit.




```{r model-categorical-no-outliers}
categoricals <- c("MAKE", "MODEL", "VEHICLECLASS", "TRANSMISSION", "FUELTYPE")
train_no_outliers[categoricals] <- lapply(train_no_outliers[categoricals], as.factor)
model_all_cat_no_ouliers <- lm(CO2EMISSIONS ~ VEHICLECLASS + TRANSMISSION + FUELTYPE +
                      ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB,
                    data = train_no_outliers)

summary(model_all_cat_no_ouliers)
```


```{r model-full-all}
model_full_all<-lm(CO2EMISSIONS ~ ., data = train)
summary(model_full_all)
```

Model with all exept model and make columns

```{r model-clean}
model_clean <- lm(CO2EMISSIONS ~ . - MODEL - MAKE-MODELYEAR , data = train)
summary(model_clean)
```

### Lasso Regression


```{r lasso}

x <- model.matrix(CO2EMISSIONS ~ ., data = train)[, -1]  # remove intercept
y <- train$CO2EMISSIONS
set.seed(123)  # for reproducibility


lasso_cv <- cv.glmnet(x, y, alpha = 1)  # alpha = 1 → Lasso

# Plot cross-validation curve
plot(lasso_cv)

```


```{r reg-full}
# Run regsubsets with up to 10 predictors (adjust as needed)
regfit_small <- regsubsets(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB ,
                           data = train, nvmax = 10)
summary(regfit_small)


```

#### regit full


regfit_full <- regsubsets(CO2EMISSIONS ~ ., data = train, nvmax = 8, really.big = TRUE)
summary(regfit_full)
NOT RUNNING (TOO BIG)




```{r regit-small}
regfit <- regsubsets(CO2EMISSIONS ~ ENGINESIZE + CYLINDERS + FUELCONSUMPTION_COMB + VEHICLECLASS + TRANSMISSION + FUELTYPE,data = train)
summary(regfit)
```

```{r reg-names}
names(regfit)
```






```{r reg-evaluate}
# Extract summary of regsubsets object
reg_summary <- summary(regfit)

# Print values with labels
cat("RSS for each model size:\n")
print(reg_summary$rss)

cat("\nAdjusted R-squared for each model size:\n")
print(reg_summary$adjr2)

cat("\nMallows' Cp for each model size:\n")
print(reg_summary$cp)

cat("\nBIC for each model size:\n")
print(reg_summary$bic)

# Best model sizes according to each metric
cat("\nModel size with lowest RSS:", which.min(reg_summary$rss), "\n")
cat("Model size with highest Adjusted R-squared:", which.max(reg_summary$adjr2), "\n")
cat("Model size with lowest Mallows' Cp:", which.min(reg_summary$cp), "\n")
cat("Model size with lowest BIC:", which.min(reg_summary$bic), "\n")

```

```{r plot-reg}
# Assuming regfit_small was created earlier:
reg_summary <- summary(regfit)

# Set up 2x2 plotting area
par(mfrow = c(2, 2))

# 1. RSS plot
plot(reg_summary$rss, type = "l", xlab = "Number of Variables", ylab = "RSS",
     main = "Residual Sum of Squares (RSS)")
points(which.min(reg_summary$rss), reg_summary$rss[which.min(reg_summary$rss)], col = "red", pch = 19)

# 2. Adjusted R² plot
plot(reg_summary$adjr2, type = "l", xlab = "Number of Variables", ylab = "Adjusted R²",
     main = "Adjusted R²")
points(which.max(reg_summary$adjr2), reg_summary$adjr2[which.max(reg_summary$adjr2)], col = "red", pch = 19)

# 3. Cp plot
plot(reg_summary$cp, type = "l", xlab = "Number of Variables", ylab = "Mallows' Cp",
     main = "Mallows' Cp")
points(which.min(reg_summary$cp), reg_summary$cp[which.min(reg_summary$cp)], col = "red", pch = 19)

# 4. BIC plot
plot(reg_summary$bic, type = "l", xlab = "Number of Variables", ylab = "BIC",
     main = "Bayesian Information Criterion (BIC)")
points(which.min(reg_summary$bic), reg_summary$bic[which.min(reg_summary$bic)], col = "red", pch = 19)
```



```{r reg-r2}
plot(regfit, scale = "r2")
```

```{r reg-adj}
plot(regfit, scale = "adjr2")
```


```{r reg-bic}
plot(regfit, scale = "bic")
```

```{r regit-model-make}
regfit_mm <- regsubsets(CO2EMISSIONS ~ . - MODEL - MAKE -TransmissionGroup, data = train)
summary(regfit_mm)
```







```{r reg-fwd-train}
regfit.fwd <- regsubsets(CO2EMISSIONS ~ ., data = train,
    method = "forward")
summary(regfit.fwd)

```

We are  getting linear dependencies and a massive number of predictors (672) and having many dummy variables (likely from categorical features like MAKE, MODEL, TRANSMISSION, etc.). We will continue with numerical predictors only.




```{r reg-num}
library(leaps)

# Step 1: Keep only numeric columns
numeric<- train[, sapply(train, is.numeric)]

# Step 2: Remove the response variable (if it's still there)
x_numeric <- subset(numeric, select = -CO2EMISSIONS)

# Step 3: Build the formula dynamically
numeric_formula <- as.formula(paste("CO2EMISSIONS ~", paste(names(x_numeric), collapse = " + ")))

# Step 4: Run regsubsets
regfit_numeric <- regsubsets(numeric_formula, data = numeric, nvmax = 10)

# Step 5: Summary and plot
reg_summary <- summary(regfit_numeric)
reg_summary

```

```{r reg_num-adj}
plot(regfit_numeric , scale = "adjr2")
```




```{r reg-fwd}
regfit.fwd_num <- regsubsets(CO2EMISSIONS ~ ., data = numeric,
    method = "forward")
summary(regfit.fwd_num)

```


## Backwards stepwise selection


```{r reg-back}
regfit.bwd <- regsubsets(CO2EMISSIONS ~ ., data = train, method = "backward")
summary(regfit.bwd)
```





## Ridge Regression

```{r ridge}

# Keep only numeric columns (excluding character/factor variables)
train_numeric <- train[, sapply(train, is.numeric)]
test_numeric <- test[, sapply(test, is.numeric)]


# Remove CO2EMISSIONS to use as the target
x <- model.matrix(CO2EMISSIONS ~ ., data = train_numeric)[, -1]  
y <- train_numeric$CO2EMISSIONS                                  

# alpha = 0 → Ridge Regression
ridge_model <- glmnet(x, y, alpha = 0)

cv_ridge <- cv.glmnet(x, y, alpha = 0)

# Best lambda
best_lambda <- cv_ridge$lambda.min
best_lambda

final_ridge <- glmnet(x, y, alpha = 0, lambda = best_lambda)

# Prepare test data the same way
x_test <- model.matrix(CO2EMISSIONS ~ ., data = test_numeric)[, -1]
y_test <- test_numeric$CO2EMISSIONS

# Predict
ridge_predictions <- predict(final_ridge, s = best_lambda, newx = x_test)

# Calculate RMSE
rmse_ridge <- sqrt(mean((ridge_predictions - y_test)^2))
rmse_ridge
```

Ridge regression applies a penalty to the size of the coefficients but does not eliminate any variables. Compared to our earlier models (e.g., multiple linear regression RMSE around 22.8 or 24.3), Ridge Regression is performing better (RMSE ≈ 19.7), likely due to its ability to handle multicollinearity and shrink unimportant features.
Our selected lambda (≈ 5.74) suggests moderate regularization 


```{r lasso-2 }

# 1. Create model matrix and response
x <- model.matrix(CO2EMISSIONS ~ ., data = train_numeric)[, -1]
y <- train_numeric$CO2EMISSIONS

# 2. Cross-validated Lasso model
cv_lasso <- cv.glmnet(x, y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
best_lambda_lasso

# 3. Fit final model with best lambda
final_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda_lasso)

# 4. Prepare test data
x_test <- model.matrix(CO2EMISSIONS ~ ., data = test_numeric)[, -1]
y_test <- test_numeric$CO2EMISSIONS

# 5. Predict
lasso_predictions <- predict(final_lasso, s = best_lambda_lasso, newx = x_test)

# 6. Evaluate performance
rmse_lasso <- sqrt(mean((lasso_predictions - y_test)^2))
rmse_lasso

```


Lasso regression performs both regularization and variable selection. With a moderate lambda (~0.38), it shrinks some coefficients exactly to zero, meaning it automatically selects only the most important variables for predicting CO₂ emissions.
Lasso achieved a slightly lower RMSE than Ridge, meaning it predicted the CO₂ emissions more accurately.


```{r plot-lasso}
lasso.mod <- glmnet(x, y, alpha = 1,
    best_lambda = grid)
plot(lasso.mod)
```

```{r plot-cv-lasso}
plot(cv_lasso)
```


```{r plot-cv-ridge}
plot(cv_ridge)
```


## PCR

```{r pcr}

# Fit PCR model with cross-validation
pcr_model <- pcr(CO2EMISSIONS ~ ., data = train_numeric, scale = TRUE, validation = "CV")

summary(pcr_model)
# Plot cross-validation RMSE
validationplot(pcr_model, val.type = "RMSEP")

# Predict on test set using optimal # of components
ncomp_pcr <- which.min(RMSEP(pcr_model)$val[1, , ])  # best number of components
pcr_pred <- predict(pcr_model, newdata = test_numeric, ncomp = ncomp_pcr)

cat("Optimal number of components for PCR:", ncomp_pcr, "\n")


# Compute RMSE
rmse_pcr <- sqrt(mean((y_test - pcr_pred)^2))
rmse_pcr
```



## Partial Least Squares (PLS) Regression

```{r PLS}
# PLS Regression with cross-validation
pls_model <- plsr(CO2EMISSIONS ~ ., data = train_numeric, scale = TRUE, validation = "CV")
validationplot(pls_model, val.type = "RMSEP", main = "PLS RMSEP Plot")

```

```{r comp-pls}
which.min(RMSEP(pls_model)$val[estimate = "CV", , ])  
summary(pls_model)
```


The optimal number of components for PLS regression is 5 ,meaning the lowest RMSEP occurred at component number 5 which the PLS model with the best cross-validated prediction accuracy.In terms of model performance, the PLS model with 5 components explained:
100% of the variance in the predictor variables, and
88.57% of the variance in CO₂ emissions.
This indicates a strong predictive capability with a well-balanced model that captures most of the important variance in both predictors and the response.

# Evaluate the performance on test set
```{r ev-pls}
# Choose number of components 
pls_pred <- predict(pls_model, newdata = test_numeric, ncomp = 5)

# Calculate RMSE
rmse_pls <- sqrt(mean((pls_pred - test_numeric$CO2EMISSIONS)^2))
rmse_pls
```

After identifying the optimal number of components (5) using cross-validation, we evaluated the PLS model on the test set. The model achieved a Root Mean Squared Error (RMSE) of 19.63, indicating strong predictive accuracy on unseen data.This result confirms that the model generalizes well beyond the training set, supporting the choice of 5 components as a good balance between bias and variance

## l1-regularization for Logistic Regression

```{r l1}


# Convert CO2EMISSIONS to binary: above/below median
threshold <- median(train_numeric$CO2EMISSIONS)
y_logit <- ifelse(train_numeric$CO2EMISSIONS > threshold, 1, 0)
y_logit <- as.factor(y_logit)

# Convert predictors to matrix
x_logit <- model.matrix(CO2EMISSIONS ~ ., data = train_numeric)[, -1]

# Fit L1-regularized logistic regression (lasso)
cv_lasso_logit <- cv.glmnet(x_logit, y_logit, alpha = 1, family = "binomial")
plot(cv_lasso_logit)
# Get best lambda
best_lambda_logit <- cv_lasso_logit$lambda.min
best_lambda_logit
final_lasso_logit <- glmnet(x_logit, y_logit, alpha = 1, lambda = best_lambda_logit, family = "binomial")


```


```{r}
x_test_logit <- model.matrix(CO2EMISSIONS ~ ., data = test_numeric)[, -1]

# Predict probabilities
pred_probs <- predict(final_lasso_logit, newx = x_test_logit, type = "response")

# Convert to class (threshold 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

actual_classes <- as.numeric(as.factor(test_numeric$target_variable)) - 1
accuracy <- mean(pred_classes == actual_classes)
accuracy

```





